{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.6439  0.6374  0.8579\n",
       " 0.4668  0.0008  0.1469\n",
       " 0.0853  0.2430  0.5965\n",
       " 0.0163  0.7560  0.4052\n",
       " 0.3702  0.6134  0.4598\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个5×3的随机矩阵并显示\n",
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor的运算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       " 1  1  1\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(5,3)\n",
    "# 创建一个5×3的全是1矩阵并显示\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.6439  1.6374  1.8579\n",
       " 1.4668  1.0008  1.1469\n",
       " 1.0853  1.2430  1.5965\n",
       " 1.0163  1.7560  1.4052\n",
       " 1.3702  1.6134  1.4598\n",
       "[torch.FloatTensor of size 5x3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算两个矩阵相加(注意尺寸要求一样)\n",
    "z = x + y\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2.1392  2.1392  2.1392  2.1392  2.1392\n",
       " 0.6145  0.6145  0.6145  0.6145  0.6145\n",
       " 0.9249  0.9249  0.9249  0.9249  0.9249\n",
       " 1.1774  1.1774  1.1774  1.1774  1.1774\n",
       " 1.4433  1.4433  1.4433  1.4433  1.4433\n",
       "[torch.FloatTensor of size 5x5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵乘法，矩阵转置\n",
    "q = x.mm(y.t())\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有Numpy上面关于ndarray的运算全部可以应用于tensor\n",
    "\n",
    "有关tensor的运算参考 http://pytorch.org/docs/master/tensors.html\n",
    "\n",
    "从numpy到tensor的转换：torch.from_numpy(a)\n",
    "\n",
    "从tensor到numpy的转换：a.numpy()\n",
    "\n",
    "tensor与numpy的最大不同：tensor可以在GPU上运算\n",
    "\n",
    "转到gpu上运算(x.cpu()转成cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.6439  1.6374  1.8579\n",
      " 1.4668  1.0008  1.1469\n",
      " 1.0853  1.2430  1.5965\n",
      " 1.0163  1.7560  1.4052\n",
      " 1.3702  1.6134  1.4598\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy桥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将Torch的Tensor和numpy的array相互转换。注意Torch的Tensor和numpy的array会共享他们的存储空间，修改一个会导致另外的一个也被修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 此处演示tensor和numpy数据结构的相互转换\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# 此处演示当修改numpy数组之后,与之相关联的tensor也会相应的被修改\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n",
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将numpy的Array转换为torch的Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)\n",
    "np.add(a,1,out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.6439  1.6374  1.8579\n",
      " 1.4668  1.0008  1.1469\n",
      " 1.0853  1.2430  1.5965\n",
      " 1.0163  1.7560  1.4052\n",
      " 1.3702  1.6134  1.4598\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 另外除了CharTensor之外，所有的tensor都可以在CPU运算和GPU预算之间相互转换\n",
    "# 使用CUDA函数来将Tensor移动到GPU上\n",
    "# 当CUDA可用时会进行GPU的运算\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态计算图(Dynamic Computation Graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 是pytorch的最主要特征\n",
    "# 让计算模型更灵活，复杂\n",
    "# 让反向传播算法随时进行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##自动微分变量\n",
    "gradient梯度，传播的就是梯度\n",
    "\n",
    "定义一个自动微分变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# Variable:自动微分变量\n",
    "x = Variable(torch.ones(2,2),requires_grad = True)\n",
    "#把一个2*2的张量转变成微分的变量（添加节点，构造计算图）\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<AddBackward0 object at 0x7f757a2c53c8>\n",
      "None\n",
      "Variable containing:\n",
      " 9\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 9\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "# 因为y是通过一个操作创建的,所以它有grad_fn,而x是由用户创建,所以它的grad_fn为None.\n",
    "print(y.grad_fn)\n",
    "print(x.grad_fn)\n",
    "\n",
    "z = torch.mean(y*y)\n",
    "print(z)\n",
    "\n",
    "z.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度(Gradients)\n",
    "现在我们来执行反向传播,out.backward()相当于执行out.backward(torch.Tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18  18\n",
      " 18  18\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 输出out对x的梯度d(out)/dx:\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更疯狂的函数依赖\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 15.3600\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = Variable(torch.FloatTensor([[0.01,0.02]]),requires_grad = True)\n",
    "x = Variable(torch.ones(2,2),requires_grad = True)\n",
    "for i in range(10):\n",
    "    s = s.mm(x) # 矩阵乘法\n",
    "z = torch.mean(s)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 37.1200  37.1200\n",
      " 39.6800  39.6800\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# backward()求导计算\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(s.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "可以使用torch.nn包来构建神经网络.\n",
    "\n",
    "你已知道autograd包,nn包依赖autograd包来定义模型并求导.一个nn.Module包含各个层和一个faward(input)方法,该方法返回output.\n",
    "\n",
    "例如,我们来看一下下面这个分类数字图像的网络.\n",
    "\n",
    "神经网络的典型训练过程如下: \n",
    "1. 定义神经网络模型,它有一些可学习的参数(或者权重); \n",
    "2. 在数据集上迭代; \n",
    "3. 通过神经网络处理输入; \n",
    "4. 计算损失(输出结果和正确值的差距大小) \n",
    "5. 将梯度反向传播会网络的参数; \n",
    "6. 更新网络的参数,主要使用如下简单的更新原则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight = weight - learning_rate * gradient \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们先定义一个网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5*5 square convolution\n",
    "        # kernel\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        # an affine operation :y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 *5 * 5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        # max pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        # If size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仅仅需要定义一个forward函数就可以了，backward会自动地生成。\n",
    "\n",
    "你可以在forward函数中使用所有的Tensor中的操作。\n",
    "\n",
    "模型中可学习的参数会由net.parameters()返回。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward的输入和输出都是autograd.Variable.注意:这个网络(LeNet)期望的输入大小是32*32.如果使用MNIST数据集来训练这个网络,请把图片大小重新调整到32*32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0524 -0.0706 -0.0473  0.1264 -0.1027 -0.0257 -0.1180  0.0642 -0.0487  0.0590\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.rand(1,1,32,32))\n",
    "out = net(input)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将所有参数的梯度缓存清零,然后进行随机梯度的的反向传播.\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "注意: torch.nn 只接受小批量的数据\n",
    "\n",
    "整个torch.nn包只接受那种小批量样本的数据，而非单个样本。 例如，nn.Conv2d能够结构一个四维的TensornSamples x nChannels x Height x Width。\n",
    "\n",
    "如果你拿的是单个样本，使用input.unsqueeze(0)来加一个假维度就可以了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
